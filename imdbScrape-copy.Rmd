---
title: 'Web scraping in R'
output:
  pdf_document:
    fig_height: 3
    fig_width: 4.5
  html_document: default
word_document: default
editor_options: 
  chunk_output_type: console
---
  
```{r, setup, include=FALSE}
library(tidyverse)
library(stringr)
library(rvest)
library(httr)

# Starter steps:
#  - install SelectorGadget in browser
#  - explore imdb.com and omdbapi.com
#  - request API key from omdbapi.com

# Becca read me
```

##Getting data from websites

#Option 1: APIs

APIs are Application Programming Interfaces, instructions for how programs should interact with your software.  For our purposes of obtaining data, APIs exist where website developers make data nicely packaged for consumption.  The language HTTP (hypertext transfer protocol) underlies APIs, and the R package `httr()` was written to map closely to HTTP with R.

Essentially you send a request to the website (server) where you want data from, and they send a response, which should contain the data (plus other stuff).

This is a really quick introduction, just to get you started and show you what's possible.  For more information, these links can be somewhat helpful:

- https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html
- https://towardsdatascience.com/functions-with-r-and-rvest-a-laymens-guide-acda42325a77
- https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/
- https://www.rstudio.com/resources/webinars/extracting-data-from-the-web-part-1/
- https://www.rstudio.com/resources/webinars/extracting-data-from-the-web-part-2/

Plus this website with a list of public APIs:

- https://github.com/toddmotto/public-apis

Here's an example of getting data from a website that attempts to make imdb movie data available as an API.

```{r, eval = FALSE}
# library(httr)  # not needed since part of tidyverse

myapikey <- "36966857"  # enter your API key for omdbapi.com (must obtain personal key)

# default is JSON = JavaScript Object Notation, which is standard data format for APIs
# - URL obtained by searching for Coco in 2017 at omdbapi.com
# - this part is optional: plot=short&r=json&
url <- str_c("http://www.omdbapi.com/?t=Coco&y=2017&apikey=", myapikey)

coco <- GET(url)   # coco holds response from server
coco               # Status of 200 is good!

details <- content(coco, "parse")   
details                             # get a list of 25 pieces of information
details$Year                        # how to access details

```

```{r, eval = FALSE}
# Build a data set for a set of movies

# Must figure out pattern in URL for obtaining different movies
#  - try searching for others
movies <- c("Coco", "Wonder+Woman", "Get+Out", 
            "The+Greatest+Showman", "Thor:+Ragnarok")
years

# Set up empty tibble
omdb <- tibble(Title = character(), Rated = character(), Genre = character(),
       Actors = character(), Metascore = double(), imdbRating = double(),
       BoxOffice = double())

# Use for loop to run through API request process 5 times,
#   each time filling the next row in the tibble
#  - can do max of 1000 GETs per day
for(i in 1:5) {
  url <- str_c("http://www.omdbapi.com/?t=",movies[i],
               "&y=2017&apikey=", myapikey)
  onemovie <- GET(url)
  details <- content(onemovie, "parse")
  omdb[i,1] <- details$Title
  omdb[i,2] <- details$Rated
  omdb[i,3] <- details$Genre
  omdb[i,4] <- details$Actors
  omdb[i,5] <- parse_number(details$Metascore)
  omdb[i,6] <- parse_number(details$imdbRating)
  omdb[i,7] <- parse_number(details$BoxOffice)   # no $ and ,'s
}

omdb

# Here's what the resulting tibble looks like:

## # A tibble: 5 x 7
##   Title    Rated Genre      Actors          Metascore imdbRating BoxOffice
## * <chr>    <chr> <chr>      <chr>               <dbl>      <dbl>     <dbl>
## 1 Coco     PG    Animation… Anthony Gonzal…        81        8.4 208487719
## 2 Wonder … PG-13 Action, A… Gal Gadot, Chr…        76        7.5 412400625
## 3 Get Out  R     Horror, M… Daniel Kaluuya…        84        7.7 175428355
## 4 The Gre… PG    Biography… Hugh Jackman, …        48        7.7 164616443
## 5 Thor: R… PG-13 Action, A… Chris Hemswort…        74        7.9 314971245

#  could use stringr functions to further organize this data - separate 
#    different genres, different actors, etc.
```

#Option 2: rvest for web scraping

If you would like to assemble data from a website with no API, you can often acquire data using more brute force methods commonly called web scraping.  Typically, this involves finding content inside HTML (Hypertext markup language) code used for creating webpages and web applications and the CSS (Cascading style sheets) language for customizing the appearance of webpages.  

There are typically three steps to scraping data with fucntions in the `rvest` library:

1. `read_html()`.  Input the URL containing the data and turn the html code into an XML file (another markup format that's easier to work with).
2. `html_nodes()`.  Extract specific nodes from the XML file by using the CSS path that leads to the content of interest.
3. `html_text()`.  Extract content of interest from nodes.  Might also use `html_table()` etc.

Here's an example of scraping movie data directly from imdb.com.  The CSS selector used in `html_nodes()` was derived using *SelectorGadget* (go to selectorgadget.com to install in your browser).  You might also consider the fun CSS Selector tutorial at http://flukeout.github.io/.  

```{r}
# 1. Download the HTML and turn it into an XML file with read_html()
#     - look at Coco page on imdb
coco <- read_html("https://www.imdb.com/title/tt2380307/")

# 2. Extract specific nodes with html_nodes()
#     - finding exact node (e.g. span.itemprop) is the tricky part.  Among all the
#       html code used to produce a webpage, where do you go to grab the content of
#       interest?
# One solution: selectorGadget 
cast <- html_nodes(coco, ".primary_photo+ td a")
cast

# 3. Extract content from nodes with html_text(), html_name(), 
#    html_attrs(), html_children(), html_table()
# Usually will still need to do some stringr adjustments
html_text(cast)
cast_adj <- str_sub(html_text(cast), 2, -2)
cast_adj
```

*Developer Tools* in your browser (under ... > More Tools) can also be used to locate the desired content among the hierarchy of HTML tags, as in cast2.  Clink on the arrow in the upper left, then click on the desired content in the webpage.  An element of html code will be highlighted in the Developer Tools; right click on this element and Copy > XPath, which can finally be pasted into html_nodes() in your R script.  Likely quite a bit of cleaning will still need to be done.

```{r}
# Developer Tools

# cast2fail <- html_nodes(coco, xpath = '//*[@id="titleCast"]/table/tbody/tr[2]/td[2]/a')

cast2 <- coco %>%
  html_nodes(xpath = '//*[@id="titleCast"]/table') %>% 
  html_text() %>%
  str_extract_all("\\n [A-Za-z].*\\n")

cast2 <- str_sub(cast2[[1]], 3, -2)
cast2
```

Another example, this time where the website already contains data in table form, so we use html_table() instead of html_nodes().

```{r}
# checkout the website below first
mpls <- read_html("https://www.bestplaces.net/climate/city/minnesota/minneapolis")
tables <- html_nodes(mpls, css = "table") 
tables  # have to guesstimate which table contains climate info
mpls_data <- html_table(tables, header = TRUE, fill = TRUE)[[1]]    # note: not html_text
as_tibble(mpls_data)
```

Your turn:

1. Use the `httr` package to build a data set from the omdb API with a different set of movies and a different set of variables than we used earlier.  If you're feeling ambitious, tidy up the resulting data set.

```{r}
url <- str_c("http://www.omdbapi.com/?t=Harry+Potter&y=2001&apikey=", myapikey)

harry <- GET(url)   # coco holds response from server
harry               # Status of 200 is good!

details <- content(harry, "parse")   
details                             # get a list of 25 pieces of information
details$Year

omdb <- tibble(Title = character(), Rated = character(), Genre = character(),
       Actors = character(), Metascore = double(), imdbRating = double(),
       BoxOffice = double())

  omdb[1,1] <- details$Title
  omdb[1,2] <- details$Rated
  omdb[1,3] <- details$Genre
  omdb[1,4] <- details$Actors
  omdb[1,5] <- parse_number(details$Metascore)
  omdb[1,6] <- parse_number(details$imdbRating)
  omdb[1,7] <- parse_number(details$BoxOffice)  
  
  omdb

```

2. Use the `rvest` package to pull off data from imdb's top grossing films released in 2017 at https://www.imdb.com/search/title?year=2017&title_type=feature&sort=boxoffice_gross_us,desc.  In particular, pick off the "gross" for each film.  If you're feeling ambitious, pick off several variables and organize as a tibble.

```{r}
  # 1. Download the HTML and turn it into an XML file with read_html()
#     - look at Coco page on imdb
sto1 <- read_html("https://www.imdb.com/search/title?year=2017&title_type=feature&sort=boxoffice_gross_us,desc")

# 2. Extract specific nodes with html_nodes()
#     - finding exact node (e.g. span.itemprop) is the tricky part.  Among all the
#       html code used to produce a webpage, where do you go to grab the content of
#       interest?
# One solution: selectorGadget 
gross <- html_nodes(sto1, ".ghost~ .text-muted+ span")
gross

title <- html_nodes(sto1, ".lister-item-header a")
title

# 3. Extract content from nodes with html_text(), html_name(), 
#    html_attrs(), html_children(), html_table()
# Usually will still need to do some stringr adjustments
gross <- html_text(gross)
title <- html_text(title)

final <- tibble(title = title, gross = gross)

```

